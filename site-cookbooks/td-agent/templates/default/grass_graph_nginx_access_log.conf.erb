# LTSV形式のログファイルを読み込む
<source>
  type tail
  id gg_nginx_access_log_tail
  format ltsv
  time_format %Y-%m-%d %H:%M:%S %z
  path /var/log/nginx/grass-graph.shitemil.works.access.log
  read_from_head true
  rotate_wait 60
  pos_file /var/log/td-agent/grass-graph.shitemil.works.access_log.pos
  tag gg.log.nginx
</source>

<match gg.log.nginx>
  type copy
  id gg_nginx_access_log_copy
  <store>
    # BigQueryの保存先テーブルを日毎に変化させる
    type record_reformer
    id gg_nginx_access_reformer
    enable_ruby true
    tag ${tag}.${time.strftime('%Y%m%d')}
  </store>
</match>

<match gg.log.nginx.**>
  type forest
  subtype bigquery
  id gg_log_nginx_forest
  <template>
    id gg_log_nginx_forest_template
    method insert

    auth_method json_key
    json_key /etc/td-agent/.keys/gcp-credential-for-fluentd-jsonkey.json

    project a-know-home
    dataset aws_centrage_nginx_log

    flush_interval 1
    buffer_chunk_records_limit 1000
    buffer_queue_limit 1024
    num_threads 16

    auto_create_table true
    table gg_nginx_access_log_${tag_parts[-1]}

    time_format %s
    time_field time
    # scheme は home.a-know.me の access log と同じなので使い回す
    schema_path /etc/td-agent/settings/nginx_access_log_schema.json

    buffer_type file
    buffer_path /var/log/td-agent/buffer/fluent.gg_log_nginx_forest_${tag_parts[-1]}.buffer     #バッファデータの出力先ファイルパス
    buffer_chunk_limit 3k     #1chunkに保存できるデータサイズ上限
    buffer_queue_limit 3      #1queueに保存できるchunk数の上限
    retry_wait 30s   #再送を実行するまでの待ち時間
    retry_limit 5    #再送実施回数
  </template>
</match>
